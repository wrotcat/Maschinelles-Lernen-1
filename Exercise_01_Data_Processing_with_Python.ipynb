{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zK29wqgvDfA4"
   },
   "source": [
    "# Exercise 01 - Data Processing with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OPqZi6Cip93p"
   },
   "source": [
    "##  Introduction\n",
    "\n",
    "Machine learning is the science of programming computers to learn from data. In order to build sophisticated machine learning models, it is important to prepare the data to learn from beforehand. This Notebook gives an introduction to the required data processing steps applied to the Titanic Survival dataset.\n",
    "\n",
    "This dataset contains demographics and passenger information from 891 of the 2224 passengers and crew on board the RMS Titanic. \n",
    "\n",
    "![RMS Titanic departing Southampton on April 10, 1912.](https://upload.wikimedia.org/wikipedia/commons/thumb/f/fd/RMS_Titanic_3.jpg/640px-RMS_Titanic_3.jpg)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OIMpw4YC9fia"
   },
   "source": [
    "## Objectives\n",
    "After this exercise you should be familiar with the following operations, which are needed when you work with data.\n",
    " * Data loading\n",
    " * Data viewing\n",
    " * Data cleaning\n",
    " * Data slicing\n",
    " * Data mapping\n",
    "\n",
    "In this exercise we'll be using mainly *pandas*  and *numpy* for data processing, *scikit-learn* for data analysis and *seaborn/matplotlib* for visualization respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UXvWz9Ihk88x"
   },
   "source": [
    "## Why do we use Python?\n",
    "* A lot of  libraries make Python applicable to every step of the data science process like\n",
    "    * data management,\n",
    "    * analytical processing, and \n",
    "    * visualization libraries.\n",
    "    \n",
    "* Python-based Jupyter Notebooks facilitate the analysis and ensure repeatability\n",
    "* Python is an easy-to-learn and readable language\n",
    "* Python is an open language with a vibrant community\n",
    " \n",
    " * **C**\n",
    " ```\n",
    " #include \"stdio.h\"\n",
    " int main() {\n",
    " printf(\"Hello World\\n\");\n",
    " }\n",
    " ```\n",
    " \n",
    " * **Python**\n",
    " ```python\n",
    " print('Hello World')\n",
    " ```\n",
    " * **C**\n",
    " ```\n",
    " #include \"stdio.h\"\n",
    " int main() {\n",
    " int x = 3\n",
    " int y = 4\n",
    " printf(\"%s\"\\n,x+y);\n",
    " }\n",
    " ```    \n",
    " * **Python**\n",
    " ```python\n",
    " x = 3\n",
    " y = 4\n",
    " print(x+y)\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_8blBjOpw4XX"
   },
   "source": [
    "## Getting Started with the Dataset\n",
    "\n",
    "For this exercise we'll be using the Titanic data set.\n",
    "It contains the passenger data from the RMS Titanic, including whether a passenger survived the sinking of the ship or not.\n",
    "The dataset can be downloaded from [Kaggle](https://www.kaggle.com/c/3136/download-all).\n",
    "\n",
    "**Our goal is to create a model to predict whether a passenger survived or not based on the given attributes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vt7u71K-h6vY"
   },
   "outputs": [],
   "source": [
    "# install a new version of Seaborn\n",
    "#!pip install -q seaborn==0.9.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vz0i7F8fqcq2"
   },
   "source": [
    "**Note:** Colab imports *seaborn* at startup, before you've pip install'd the new version. Hence you need to restart your runtime after the install, to pick up the new version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qk9FDv1RXeL_"
   },
   "outputs": [],
   "source": [
    "# Import required Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "2aRtIMk7oPJv",
    "outputId": "9fd87bc0-e744-4d65-ea8f-5eafcb3cb137"
   },
   "outputs": [],
   "source": [
    "# Test whether the version of Seaborn is 0.9.0\n",
    "if (sns.__version__ == '0.9.0'):\n",
    "  print('correct seaborn version is loaded ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DqZatG0ArZVS"
   },
   "source": [
    "**If you are using Google Colab, you have to upload the dataset with the following command: **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": "OK"
      }
     }
    },
    "colab_type": "code",
    "id": "sTtRNOPNqTWU",
    "outputId": "3ec61765-062b-465b-b59b-f55b2fd8dda9"
   },
   "outputs": [],
   "source": [
    "# Uplod dataset on the cloud\n",
    "# from google.colab import files\n",
    "# files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tJDg1bP9rUpU"
   },
   "source": [
    "Now we can use the dataset and start analyzing it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "id": "wnlLcan3rUZ1",
    "outputId": "f3130b02-70d8-4bf8-b08b-ab07327b91aa"
   },
   "outputs": [],
   "source": [
    "# Load the train and test datasets from the CSV files\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "# Combine the datasets for training and testing to one full data set\n",
    "full_data = [train, test]\n",
    "\n",
    "# Display the first 5 rows of the train data set\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "o6-WdtKMEG1g",
    "outputId": "a1bbf470-da26-4abc-d2b0-0ee7b71effe6"
   },
   "outputs": [],
   "source": [
    "# Print the columns of the data frame\n",
    "train.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iaXUuHabtWCq"
   },
   "source": [
    "\n",
    " *  **Survived**: Outcome of survival (int: 0 = No; 1 = Yes) \n",
    " *  **Pclass**: Socio-economic class (int: 1 = Upper class; 2 = Middle class; 3 = Lower class)\n",
    " * **Name**: Name of passenger (string)\n",
    " * **Sex**: Sex of the passenger (string)\n",
    " * **Age**: Age of the passenger (float: Some entries contain NaN)\n",
    " * **SibSp**: Number of siblings and spouses of the passenger aboard (int)\n",
    " * **Parch**: Number of parents and children of the passenger aboard (int)\n",
    " * **Ticket**: Ticket number of the passenger (string)\n",
    " * **Fare**: Fare paid by the passenger (float)\n",
    " * **Cabin** Cabin number of the passenger (string: Some entries contain NaN)\n",
    " * **Embarked**: Port of embarkation of the passenger (string: C = Cherbourg; Q = Queenstown; S = Southampton)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "colab_type": "code",
    "id": "DylW5_AK7wlh",
    "outputId": "8c236a10-c7b0-42db-bff0-f27b64a3195e"
   },
   "outputs": [],
   "source": [
    "# Inspect the data, *info* can be used to show how complete or incomplete the\n",
    "# dataset is\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eGitn_m7HDkv"
   },
   "source": [
    "To display information about a specific passenger, we can select a row with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "colab_type": "code",
    "id": "J_az9Iv5Hhka",
    "outputId": "63a84cf7-c1c4-420a-8628-c9d53ddb39a4"
   },
   "outputs": [],
   "source": [
    "# iloc: index location\n",
    "train.iloc[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "id": "HxzMR70EqVkt",
    "outputId": "ea9ded2d-fbd7-4736-beaa-23a950d2304b"
   },
   "outputs": [],
   "source": [
    "# Retrieve n number of samples from the data set\n",
    "train.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "colab_type": "code",
    "id": "jWuN8vc-sYWI",
    "outputId": "f17de49f-31b4-4bbe-b41e-f11bd260234a"
   },
   "outputs": [],
   "source": [
    "# Retrieve a statistical description of the data set\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YuJfd4rzDgf5"
   },
   "source": [
    "Above we can see that 38% out of the training-set survived the sinking of the RMS Titanic. We can also see that the passenger ages range from 0.4 years to 80 years.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vYAMq-gZDrI_"
   },
   "source": [
    "From the tables above, we can note a few things:\n",
    "1. We need to convert a lot of features into numeric ones\n",
    "2. We can see that the features have widely different ranges\n",
    "3. We can detect some features, that contain missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y14A5NPWwcXh"
   },
   "source": [
    "## Visualizing the Data\n",
    "\n",
    "After a look at the tables and the description of our datasets we can't say a lot about the data yet. We are unaware of the distribution and correlation of the variables regarding the chances of survival for any given passenger.\n",
    "To get a better understanding about the dataset and its variables, it is helpful to visualize it.\n",
    "\n",
    "A good tool to visualize data using Python is the library *matplotlib*. It is well [documented](https://matplotlib.org/) and allows for extensive customizability.\n",
    "\n",
    "Additionally we will introduce [*seaborn*](https://seaborn.pydata.org/), a wrapper which uses matplotlib, but offers a higher-level interface for visualizing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "colab_type": "code",
    "id": "nEiIu-zHzp9r",
    "outputId": "3855b8f5-f952-46eb-9450-201f70734c7b"
   },
   "outputs": [],
   "source": [
    "# First we start off with matplotlib and setup the figures and plots\n",
    "f,ax = plt.subplots(1,3,figsize=(20,5))\n",
    "colors = [\"r\", \"g\", \"b\"]\n",
    "\n",
    "# Now we look at some general distributions of the data.\n",
    "\n",
    "\n",
    "# Survived Class\n",
    "x_survived = [0,1]\n",
    "y_survived = [np.where(train[\"Survived\"] == (i))[0].size for i in x_survived]\n",
    "ax[0].bar(x_survived, y_survived, color=colors)\n",
    "ax[0].set_xticks(x_survived)\n",
    "ax[0].set_title('Survived')\n",
    "\n",
    "\n",
    "# Passenger Class\n",
    "x_pclass = [1, 2, 3]\n",
    "y_pclass = [np.where(train[\"Pclass\"] == (i))[0].size for i in x_pclass]\n",
    "ax[1].bar(x_pclass, y_pclass, color=colors)\n",
    "ax[1].set_xticks(x_pclass)\n",
    "ax[1].set_title('Passengers by Class')\n",
    "\n",
    "\n",
    "# Age\n",
    "x_age = np.arange(0, 80)\n",
    "y_age = [np.where(train[\"Age\"] == (i))[0].size for i in x_age]\n",
    "ax[2].bar(x_age, y_age, color=\"green\")\n",
    "ax[2].set_xticks(np.arange(0, 81, 10))\n",
    "ax[2].set_title('Passengers by Age')\n",
    "\n",
    "\n",
    "# Display the graphs\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "colab_type": "code",
    "id": "j-MqnfqNKAQO",
    "outputId": "d300085e-b774-4e54-a21d-735ce997abcd"
   },
   "outputs": [],
   "source": [
    "# The categorical histograms we created with matplotlib can be created with the\n",
    "# \"countplot\" command in seaborn. The setup is very similar, but easier.\n",
    "\n",
    "f,ax = plt.subplots(1,3,figsize=(20,5))\n",
    "\n",
    "sns.countplot('Survived',data=train,ax=ax[0])\n",
    "ax[0].set_title('Survived')\n",
    "\n",
    "sns.countplot('Pclass',data=train,ax=ax[1])\n",
    "ax[1].set_title('Passengers by Class')\n",
    "\n",
    "sns.distplot(train['Age'].dropna(),ax=ax[2],bins=20, kde = False)\n",
    "ax[2].set_title('Passengers by Age');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "colab_type": "code",
    "id": "skTdJokPL0MY",
    "outputId": "ee9fdb3e-0170-4882-9771-88406b2a0308"
   },
   "outputs": [],
   "source": [
    "# Now we are going to use various plot options of seaborn to \n",
    "# look at survival rates based on other attributes:\n",
    "f,ax = plt.subplots(1,3,figsize=(20,5))\n",
    "\n",
    "sns.countplot('Pclass',hue='Survived',data=train,ax=ax[0])\n",
    "ax[0].set_title('Survival by Class')\n",
    "\n",
    "sns.countplot('Sex',hue='Survived',data=train,ax=ax[1])\n",
    "ax[1].set_title('Survival by Sex')\n",
    "\n",
    "sns.countplot('Embarked',hue='Survived',data=train,ax=ax[2])\n",
    "ax[2].set_title('Survival by Embarked');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "colab_type": "code",
    "id": "iWeax6Buxlf6",
    "outputId": "3d9fd2cd-d29c-4e02-aa1e-47e1361052a3"
   },
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(1,3,figsize=(20,5))\n",
    "\n",
    "sns.countplot('SibSp',hue='Survived',data=train,ax=ax[0])\n",
    "ax[0].set_title('Survival by SibSp')\n",
    "\n",
    "sns.countplot('Parch',hue='Survived',data=train,ax=ax[1])\n",
    "ax[1].set_title('Survival by Parch')\n",
    "\n",
    "sns.distplot(train[train['Survived']==0]['Fare'].dropna(),ax=ax[2],kde=False,color='red',bins=20)\n",
    "sns.distplot(train[train['Survived']==1]['Fare'].dropna(),ax=ax[2],kde=False,color='blue',bins=20)\n",
    "ax[2].set_title('Survival by Fare');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3zoUW2hR06kk"
   },
   "source": [
    "This was only a quick overview of the relationship between features before we start a more detailed analysis in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "giXF24ryxChi"
   },
   "source": [
    "## Cleaning the Data\n",
    "Data from the real world is messy. Normally there are missing values, outliers and invalid data (e.g. negative values for age) in a data set. We can solve problems with data quality by replacing these values, trying to close the gap by interpolation or by dropping the respective entries. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u8BFJS3h3hWS"
   },
   "source": [
    "### Detecting and Filtering Outliers\n",
    "\n",
    "Outliers that are either very large or small skew the overall view of the data. One way of detecting outliers could be the use of the standard deviation. If we assume that the data is normally distributed, then 95 percent of the data is within 1.96 standard deviations of the mean. So we can drop the values either above or below that range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "colab_type": "code",
    "id": "BsizpojeSCy-",
    "outputId": "06cd6d32-12d8-40f9-fd13-8ee37571dd43"
   },
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(1,2,figsize=(13,5))\n",
    "\n",
    "# The outliers in Fare (Fare paid by the passenger)\n",
    "sns.regplot(x=train[\"PassengerId\"], y=train[\"Fare\"], fit_reg=False,ax=ax[0])\n",
    "\n",
    "# The outliers in SibSp(Number of siblings and spouses of the passenger aboard)\n",
    "sns.regplot(x=train[\"PassengerId\"], y=train[\"SibSp\"], fit_reg=False, ax=ax[1])\n",
    "\n",
    "ax[0].set_title('Passengers by Fare')\n",
    "ax[1].set_title('Passengers by Number of Siblings and Spouses')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ivIh8wNH5ZS-"
   },
   "outputs": [],
   "source": [
    "# Outlier detection  Method 1 using Standard Deviation\n",
    "def detect_outliers_sd(df,n,features):\n",
    "    outlier_indices = []\n",
    "    # iterate over features(columns)\n",
    "    for col in features:\n",
    "        # mean\n",
    "        mean = df[col].mean()\n",
    "        # standard deviation\n",
    "        std = df[col].std()\n",
    "        # the upper bound\n",
    "        top = mean + std * 1.96\n",
    "        #  the lower bound \n",
    "        bot = mean - std * 1.96\n",
    "        # Determine a list of indices of outliers for feature col\n",
    "        outlier_list_col = df[(df[col] < bot) | (df[col] > top)].index       \n",
    "        # append the found outlier indices for col to the list of outlier indices \n",
    "        outlier_indices.extend(outlier_list_col)\n",
    "    # select observations containing more than 2 outliers\n",
    "    outlier_indices = Counter(outlier_indices)        \n",
    "    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n",
    "    return multiple_outliers   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U_4-MN9wkQ0i"
   },
   "outputs": [],
   "source": [
    "# Outlier detection  Method 2 using Interquartile Ranges \n",
    "def detect_outliers_iqr(df, n ,features):\n",
    "\n",
    "    outlier_indices = []\n",
    "\n",
    "    # iterate over features(columns)\n",
    "    for col in features:\n",
    "        # 1st quartile (25%)\n",
    "        Q1 = np.percentile(df[col], 25)\n",
    "        # 3rd quartile (75%)\n",
    "        Q3 = np.percentile(df[col] ,75)\n",
    "        # Interquartile range (IQR)\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        # outlier step\n",
    "        outlier_step = 1.5 * IQR\n",
    "\n",
    "        # Determine a list of indices of outliers for feature col\n",
    "        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n",
    "\n",
    "        # append the found outlier indices for col to the list of outlier indices\n",
    "        outlier_indices.extend(outlier_list_col)\n",
    "\n",
    "    # select observations containing more than 2 outliers\n",
    "    outlier_indices = Counter(outlier_indices)\n",
    "    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n",
    "    return multiple_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F_ias0WFXDLe"
   },
   "source": [
    "Detect outliers ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 398
    },
    "colab_type": "code",
    "id": "0wEro4QuXGcC",
    "outputId": "29242551-1704-451a-f555-37a059321b20"
   },
   "outputs": [],
   "source": [
    "# detect outliers from Age, SibSp , Parch and Fare\n",
    "outliers_to_drop = detect_outliers_iqr(train,2,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"])\n",
    "train.loc[outliers_to_drop] # Show the outliers rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mz8vBJj7DeI_"
   },
   "source": [
    "... and remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_2-FKODKDgFU"
   },
   "outputs": [],
   "source": [
    "# Drop the outliers\n",
    "train = train.drop(outliers_to_drop, axis = 0).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wpH-T3bJgniP"
   },
   "source": [
    "### Complementary functions\n",
    "\n",
    "Most\tMachine\tLearning\talgorithms\tcannot\twork\twith missing values,\tso\tletâ€™s\tcreate a few\tfunctions\tto take\tcare\tof\tthe missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "colab_type": "code",
    "id": "SfBr9uVKGFPT",
    "outputId": "ceaee033-b724-4708-80f5-e32dbdd7cf14"
   },
   "outputs": [],
   "source": [
    "# The .info function shows how complete or incomplete the datasets are. \n",
    "print(train.isnull().sum())\n",
    "print('-'*30)\n",
    "print(test.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UitahntAhiHd"
   },
   "source": [
    "We can complete missing data by calculating:\n",
    "* mean, \n",
    "* median, or \n",
    "* mean + randomized standard deviation.  \n",
    "\n",
    "Before we can complete the missing data, we should decide which method is best based on the description of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "LNzeuht1ieHx",
    "outputId": "4a3184f5-8bff-46bb-ebf4-d69cc06aae5e"
   },
   "outputs": [],
   "source": [
    "# The outliers in Age \n",
    "sns.regplot(x=train[\"PassengerId\"], y=train[\"Age\"], fit_reg=False)\n",
    "plt.title('Age by Passenger')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MzeUJIKAieqC"
   },
   "source": [
    "To complete the missing data of *Age*, we use the mean + randomized standard deviation, where the standard deviation describes the spread of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KXtcknVdvR1S"
   },
   "outputs": [],
   "source": [
    "# Fill the missing data in Age using mean + randomized standard deviation. \n",
    "for dataset in full_data:\n",
    "    age_avg = dataset['Age'].mean()\n",
    "    age_std = dataset['Age'].std()\n",
    "    age_null_count = dataset['Age'].isnull().sum()\n",
    "    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)  \n",
    "    df_age = dataset['Age'].copy()\n",
    "    df_age[np.isnan(df_age)] = [age_null_random_list]\n",
    "    dataset['Age'] = df_age.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KzGCyBiTgBju"
   },
   "source": [
    "Information of the Fare attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "Qw0C1tf1hFFl",
    "outputId": "58c02259-fba9-4cd1-ed0e-ccf0740d6d82"
   },
   "outputs": [],
   "source": [
    "# The Description of Fare\n",
    "print (\"median {}\".format(train['Fare'].median()))\n",
    "train['Fare'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "miVPP18CoXeb"
   },
   "outputs": [],
   "source": [
    "# Fill the missing data in Fare using median standard deviation.\n",
    "for dataset in full_data:\n",
    "    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QWfpBgx-uu2O"
   },
   "source": [
    "Before we fill the missing data in the Embarked, we will visualize it to decide which option is best to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369
    },
    "colab_type": "code",
    "id": "F5BiIfhHkTVs",
    "outputId": "4d118757-2251-4b58-fba0-35d51737f4f8"
   },
   "outputs": [],
   "source": [
    "sns.countplot('Embarked',data=train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2C49tw7lkM-J"
   },
   "outputs": [],
   "source": [
    "for dataset in full_data:\n",
    "    dataset['Embarked'] = dataset['Embarked'].fillna('S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "col3zTi-8GPl"
   },
   "source": [
    "Let us check again if there are any missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "colab_type": "code",
    "id": "hWrXD-jplyxw",
    "outputId": "9b1395c3-912b-4e35-a278-ca5443bc150b"
   },
   "outputs": [],
   "source": [
    "# update the dataframes\n",
    "train = full_data[0]\n",
    "test = full_data[1]\n",
    "# any: detects if a cell matches a condition\n",
    "print(train.isnull().any())\n",
    "print('-'*30)\n",
    "print(test.isnull().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bYVF47Bq8kGg"
   },
   "source": [
    "Great! Nothing (important) is missing, and we did not have to remove any rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DdEJv3_FsnS8"
   },
   "source": [
    "## Feature Engineering\n",
    "Qualitative data is often nominal (e.g. names) or categorical (e.g. sex). Those can't be ordered and are difficult to evaluate. Therefore we want to convert all our categorial data to quantitiative data, i.e. numerical or ordinal values.\n",
    "\n",
    "We can convert the names to an attribute based on their length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "kp_h3nQpv6GK",
    "outputId": "495d8c56-8597-48e2-dbd4-d6117ca602f3"
   },
   "outputs": [],
   "source": [
    "for dataset in full_data:\n",
    "    try:\n",
    "        dataset['Name_length'] = dataset['Name'].apply(len)\n",
    "    except:\n",
    "        print(\"Name_length feature is located in the data frame\")\n",
    "        \n",
    "train['Name_length'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 622
    },
    "colab_type": "code",
    "id": "1pjrAdvP2vpt",
    "outputId": "be665fa0-9ea5-42b2-aac4-b519eecae59f"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1,figsize=(20,10))\n",
    "\n",
    "# The amount of survived people by Name length.\n",
    "sum_survived_by_name = train[[\"Name_length\", \"Survived\"]].groupby(['Name_length'],as_index=False).sum()\n",
    "sns.barplot(x='Name_length', y='Survived', data=sum_survived_by_name, ax = ax[0])\n",
    "ax[0].set_title('Survivors by Name length')\n",
    "\n",
    "# The amount of survived people by Name length.\n",
    "avg_survived_by_name = train[[\"Name_length\", \"Survived\"]].groupby(['Name_length'],as_index=False).mean()\n",
    "sns.barplot(x='Name_length', y='Survived', data=avg_survived_by_name, ax = ax[1])\n",
    "ax[1].set_title('Survival Rates by Name length')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UVsl4n6Mwve6"
   },
   "source": [
    "From the graphics above we can see that passengers with longer names were more likely to survive, perhaps the cause is that rich families tend to have longer names.\n",
    "\n",
    "It can also be helpful to create meaningful \"bins\" for attributes. Therefore we will divide the Name_length feature into small classes. Each of these classes has a similar rate to survive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "id": "XApLwZoFd5Kw",
    "outputId": "68992d9b-e8cf-4909-a254-2633ecd0522f"
   },
   "outputs": [],
   "source": [
    "for dataset in full_data:\n",
    "    dataset.loc[ dataset['Name_length'] <= 23, 'Name_length']= 0\n",
    "    dataset.loc[(dataset['Name_length'] > 23) & (dataset['Name_length'] <= 28), 'Name_length']= 1\n",
    "    dataset.loc[(dataset['Name_length'] > 28) & (dataset['Name_length'] <= 40), 'Name_length']= 2\n",
    "    dataset.loc[ dataset['Name_length'] > 40, 'Name_length'] = 3\n",
    "train['Name_length'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IrEuhSrr17Wn"
   },
   "source": [
    "As a next step we can map categorical attributes to a numerical discrete value:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "oU2asfKFsdFV",
    "outputId": "f8663a23-9940-475c-dc4d-288b0676bf4e"
   },
   "outputs": [],
   "source": [
    "# Mapping Gender\n",
    "for dataset in full_data:\n",
    "    # np.where takes as input a list of Booleans, a new value and a backup value\n",
    "    try:\n",
    "        dataset['Sex'] = np.where(dataset['Sex']=='female', 1, 0)\n",
    "    except:\n",
    "        print('The value is already converted ')\n",
    "train['Sex'].head()\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g2XE-01iQnuJ"
   },
   "source": [
    "For example we can look at the *Age* attribute:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "colab_type": "code",
    "id": "Eqtb9XMGRXBC",
    "outputId": "9b30bbd3-0fd5-46e2-aa97-e5a219368263"
   },
   "outputs": [],
   "source": [
    "#plot distributions of passengers who survived or died by age\n",
    "a = sns.FacetGrid(train, hue='Survived', aspect=5)\n",
    "a.map(sns.kdeplot, 'Age', shade=True)\n",
    "a.set(xlim=(0, train['Age'].max()))\n",
    "a.add_legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-87Je5HFRjCW"
   },
   "source": [
    "We can see that until the age of 14 the chance of survival is higher than the chance to die.\n",
    "In reverse the chance for dying is higher between the age of 14 and 30. This changes a couple of times between various ages.\n",
    "\n",
    "Therefore the best categories for age are:\n",
    "* 0: less than 14\n",
    "* 1: 14 to 30\n",
    "* 2: 30 to 40\n",
    "* 3: 40 to 50\n",
    "* 4: 50 to 60\n",
    "* 5: 60 and more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "x6E4P9AiS6kK",
    "outputId": "f2c54e05-a148-4ff7-def6-0ca913467972"
   },
   "outputs": [],
   "source": [
    "for dataset in full_data:\n",
    "    dataset.loc[ dataset['Age'] <= 14, 'Age_bin'] = 0\n",
    "    dataset.loc[(dataset['Age'] > 14) & (dataset['Age'] <= 30), 'Age_bin'] = 1\n",
    "    dataset.loc[(dataset['Age'] > 30) & (dataset['Age'] <= 40), 'Age_bin'] = 2\n",
    "    dataset.loc[(dataset['Age'] > 40) & (dataset['Age'] <= 50), 'Age_bin'] = 3\n",
    "    dataset.loc[(dataset['Age'] > 50) & (dataset['Age'] <= 60), 'Age_bin'] = 4\n",
    "    dataset.loc[ dataset['Age'] > 60, 'Age_bin'] = 5\n",
    "train['Age_bin'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fVYLOXySz2w8"
   },
   "source": [
    "The next step is to map the Embarked feature: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "iZvReH5r1hBJ",
    "outputId": "f8932a39-fc1b-4ea5-8a87-0047aca7c1e6"
   },
   "outputs": [],
   "source": [
    "# Mapping Embarked\n",
    "for dataset in full_data:\n",
    "  try:\n",
    "      dataset.Embarked.replace(('S','C','Q'), (0,1,2), inplace = True)\n",
    "  except:\n",
    "      print('The value is already converted ')\n",
    "train['Embarked'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zHvkbWfoOWxd"
   },
   "source": [
    "Additionally data might be skewed. For example, if we look at the *Fare* attribute, we can see it is heavily skewed to the left:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 355
    },
    "colab_type": "code",
    "id": "7DToPJjpOhEi",
    "outputId": "51bd9fb5-6f81-47f0-d5a2-84a7b5f4b944"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(18,5))\n",
    "sns.distplot(train[\"Fare\"][train[\"Survived\"] == 0], color=\"red\")\n",
    "sns.distplot(train[\"Fare\"][train[\"Survived\"] == 1], color=\"blue\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3g7xPY-hOst-"
   },
   "source": [
    "To reduce the skewness of this attribute, we can transform it with the log function. This redistributes the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "colab_type": "code",
    "id": "rrj7LT3hOsfr",
    "outputId": "27448969-d919-40a6-c268-3e98958a60dc"
   },
   "outputs": [],
   "source": [
    "# Apply log to Fare to reduce skewness distribution\n",
    "for dataset in full_data:\n",
    "    dataset[\"Fare_log\"] = dataset[\"Fare\"].map(lambda i: np.log(i) if i > 1 else 0)\n",
    "    \n",
    "fig, ax = plt.subplots(figsize=(18,5))\n",
    "sns.distplot(train[\"Fare_log\"][train[\"Survived\"] == 0], color=\"r\", ax=ax)\n",
    "sns.distplot(train[\"Fare_log\"][train[\"Survived\"] == 1], color=\"b\", ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kd3UyTHcQcX7"
   },
   "source": [
    "Now we can define bins more easily:\n",
    "The survival rate is lower for a *Fare_log* value of less than 2.7 and higher for values greater than 2.7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "bifjkPp3Q3tc",
    "outputId": "e1a79833-aa3f-48e6-f20a-009800b8431f"
   },
   "outputs": [],
   "source": [
    "for dataset in full_data:\n",
    "    dataset.loc[ dataset['Fare_log'] <= 2.7, 'Fare_bin'] = 0\n",
    "    dataset.loc[ dataset['Fare_log'] > 2.7, 'Fare_bin'] = 1\n",
    "    dataset['Fare_bin'] = dataset['Fare_bin'].astype(int)\n",
    "train['Fare_bin'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "id": "rdU0Nmb_XecL",
    "outputId": "10555259-19fd-4881-ca10-3965d054762e"
   },
   "outputs": [],
   "source": [
    "# Print the first 5 rows of the updated dataset\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lG5vl9f1xaNA"
   },
   "source": [
    "## Feature Selection\n",
    "\n",
    "Feature selection methods can be used to identify and remove unneeded, irrelevant and redundant attributes from data that do not contribute to the accuracy of a predictive model or may in fact decrease the accuracy of the model.\n",
    "\n",
    "Fewer attributes are desirable because it reduces the complexity of the model, and a simpler model is simpler to understand and explain.\n",
    "\n",
    "**Which features within the dataset contribute significantly to our goal?  ** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QfHkXaYW0klN"
   },
   "source": [
    "To calculate the covariance matrix, we should first remove all remaining string attributes:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "colab_type": "code",
    "id": "iLzH9w5DhD21",
    "outputId": "a61e37bb-2e48-4559-f5bd-9588c5170ce9"
   },
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BurEf1fw0p7N"
   },
   "source": [
    "we will drop the following features:\n",
    "* Name\n",
    "* Ticket\n",
    "* Cabin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "YUyLQJbP2Ax_",
    "outputId": "b7de6a82-d4f0-4174-abdd-1fcbbf4bf5f7"
   },
   "outputs": [],
   "source": [
    "# Feature selection\n",
    "drop_elements = [ 'Name', 'Ticket', 'Cabin']\n",
    "try: \n",
    "  train = train.drop(drop_elements, axis = 1)\n",
    "  test  = test.drop(drop_elements, axis = 1)\n",
    "except:\n",
    "  print(\"The features are already removed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MNVM37lk1tTI"
   },
   "source": [
    "We can examine the data after removing the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "id": "Q3548CX02Cv7",
    "outputId": "dd0a665c-9ddb-432d-9ce7-9ccbef8eaca2"
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5WiNx0HR-tAm"
   },
   "source": [
    "### Correlation analysis - Multi-variate analysis\n",
    "* Basically, correlation measures how closely two variables move in the same direction. Therefore we try to find whether there is a correlation between a feature and a label. In other words as the feature values change does the label change as well, and vice-versa?\n",
    "\n",
    "* The data may contain a lot of information redundancy distributed among multiple variables, which is a problem called multivariate correllation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sqH8oCnim09-"
   },
   "source": [
    "#### Heatmap for the correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 778
    },
    "colab_type": "code",
    "id": "2ke6Hm3cU17M",
    "outputId": "b6ebafcf-f7db-4bd3-ab2d-bccdf47ac195"
   },
   "outputs": [],
   "source": [
    "colormap = plt.cm.RdBu\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
    "sns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i4_YrzES5Dat"
   },
   "source": [
    "We can see from the survived column, that it has strong relation with sex and potential relation with class (or fare)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CZ4EFFaMqQN2"
   },
   "source": [
    "### Features Selection\n",
    "We will drop the features that are not correlated with our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_XkxIqBh2kEY"
   },
   "outputs": [],
   "source": [
    "# Feature selection\n",
    "drop_elements = ['PassengerId', 'SibSp', 'Age_bin','Embarked']\n",
    "try: \n",
    "  train = train.drop(drop_elements, axis = 1)\n",
    "  test  = test.drop(drop_elements, axis = 1)\n",
    "except:\n",
    "  print(\"The features are already removed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5KuVzL67Be8"
   },
   "source": [
    "Exploring the dataset after removing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "id": "5r4IKqad2jaX",
    "outputId": "01cf9d90-1c72-4fcd-b931-cd4c61d516e2"
   },
   "outputs": [],
   "source": [
    "test.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aZzRzDSV1sIV"
   },
   "source": [
    "## Predictive Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BJFKuMh3OvHx"
   },
   "outputs": [],
   "source": [
    "import sklearn # Collection of machine learning algorithms\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-cwc2VjG7gLL"
   },
   "source": [
    "\n",
    "### Implementation: Shuffle and Split Data\n",
    "\n",
    "The next step requires that we take the train dataset and split the data into training and testing subsets. We should do this because we want to test how well our model generalizes to unseen data.\n",
    "\n",
    "Use `train_test_split` from `sklearn.cross_validation` to shuffle and split the data into training and testing sets.\n",
    "* Split the data into 70% training and 30% testing.\n",
    "* Set the *random_state* for train_test_split to 101. This ensures results are consistent over multiple runs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m8lwRPVajSqe"
   },
   "outputs": [],
   "source": [
    "X_train = train.drop(\"Survived\", axis=1)\n",
    "Y_train = train[\"Survived\"]\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, x_test, Y_train, y_test = train_test_split(X_train, Y_train, test_size=0.3, random_state=101)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x7FdSgBD3lXs"
   },
   "source": [
    "* Accuracy Function\n",
    "* Logistic Regression\n",
    "* SVM\n",
    "* Decision Tree / Random Forest\n",
    "* Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UO9MTEi2Edrd"
   },
   "source": [
    "###  Logistic Regression\n",
    "\n",
    "Logistic regression is machine learning classification algorithm that is used to predict the probability of a categorical dependent variable. In logistic regression, the dependent variable is a binary variable that contains data coded as 1 (survived) or 0 (not survived). In other words, the logistic regression model predicts P(Y=1) as a function of X (Features). This makes it a  binary classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "IdVXnFdp3kYI",
    "outputId": "cd718a89-1653-46c4-9c6c-007a9e4e3822"
   },
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(solver=\"newton-cg\")\n",
    "logreg.fit(X_train, Y_train)\n",
    "Y_pred1 = logreg.predict(x_test)\n",
    "print(\"The accuracy is {}%\".format(round(logreg.score(x_test, y_test) * 100, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eXTT_7pMPOim"
   },
   "source": [
    "### Decision Tree\n",
    "Decision tree classifiers are attractive models if we care about interpretability. As\n",
    "the name decision tree suggests, we can think of this model as breaking down our\n",
    "data by making decisions based on asking a series of questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "VcxDvyz3PQO1",
    "outputId": "7ead182f-7d3c-4b0b-aec4-7ac84b533196"
   },
   "outputs": [],
   "source": [
    "decision_tree = DecisionTreeClassifier()\n",
    "decision_tree.fit(X_train, Y_train)\n",
    "Y_pred7 = decision_tree.predict(x_test)\n",
    "print(\"The accuracy is {}%\".format(round(decision_tree.score(x_test, y_test) * 100, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6iafhmkIQBpP"
   },
   "source": [
    "### Perceptron\n",
    "The perceptron is a supervised binary classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "1Ri312l1QSnr",
    "outputId": "94cf6b6c-93e2-4108-e195-e23566286cbe"
   },
   "outputs": [],
   "source": [
    "perceptron = Perceptron()\n",
    "perceptron.fit(X_train, Y_train)\n",
    "Y_pred4 = perceptron.predict(x_test)\n",
    "print(\"The accuracy is {}%\".format(round(perceptron.score(x_test, y_test) * 100, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dDXM0aksO_3y"
   },
   "source": [
    "### Support Vector Machines \n",
    "Support Vector Machines (SVM) are kernel based methods that require only a user-specified kernel function $K$ i.e., a similarity function over pairs of data points into a kernel (dual) space on which the learning algorithms operate linearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "LCfytQrZO4qV",
    "outputId": "75398664-f0ca-41da-b486-4a034b9e88de"
   },
   "outputs": [],
   "source": [
    "svc=SVC(gamma=\"auto\")\n",
    "svc.fit(X_train, Y_train)\n",
    "Y_pred2 = svc.predict(x_test)\n",
    "print(\"The accuracy is {}%\".format(round(svc.score(x_test, y_test) * 100, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JvX9vSIkCitD"
   },
   "source": [
    "# Conclusion\n",
    "*  Machine Learning is about algorithms that are capable to learn from data, instead of having to explicitly code rules.\n",
    "* In an ML project you gather data in a training set, and you feed the training set to a learning algorithm.\n",
    "* The system will not perform well if your training set is too small, or if the data is not representative, noisy, or polluted with irrelevant features (garbage in, garbage out). \n",
    "* Lastly, your model needs to be neither too simple (in which case it will underfit) nor too complex (in which case it will overfit)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Exercise 01 - Data Processing with Python",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
